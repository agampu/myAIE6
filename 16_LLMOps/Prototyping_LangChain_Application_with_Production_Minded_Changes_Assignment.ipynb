{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZsP-j7w3zcL"
      },
      "source": [
        "# Prototyping LangChain Application with Production Minded Changes\n",
        "\n",
        "For our first breakout room we'll be exploring how to set-up a LangChain LCEL chain in a way that takes advantage of all of the amazing out of the box production ready features it offers.\n",
        "\n",
        "We'll also explore `Caching` and what makes it an invaluable tool when transitioning to production environments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpeN9ND0HKa0"
      },
      "source": [
        "## Task 1: Dependencies and Set-Up\n",
        "\n",
        "Let's get everything we need - we're going to use very specific versioning today to try to mitigate potential env. issues!\n",
        "\n",
        "> NOTE: If you're using this notebook locally - you do not need to install separate dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0P4IJUQF27jW"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU langchain_openai==0.2.0 langchain_community==0.3.0 langchain==0.3.0 pymupdf==1.24.10 qdrant-client==1.11.2 langchain_qdrant==0.1.4 langsmith==0.1.121 langchain_huggingface==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYcWLzrmHgDb"
      },
      "source": [
        "We'll need an HF Token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ8qfrFh_6ed",
        "outputId": "4fb1a16f-1f71-4d0a-aad4-dd0d0917abc5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HF Token Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piz2DUDuHiSO"
      },
      "source": [
        "And the LangSmith set-up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLZX5zowCh-q",
        "outputId": "565c588a-a865-4b86-d5ca-986f35153000"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE6 Session 16 Geeta - {uuid.uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmwNTziKHrQm"
      },
      "source": [
        "Let's verify our project so we can leverage it in LangSmith later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6GZmkVkFcHq",
        "outputId": "f4c0fdb3-24ea-429a-fa8c-23556cb7c3ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIE6 Session 16 Geeta - b2b213a6\n"
          ]
        }
      ],
      "source": [
        "print(os.environ[\"LANGCHAIN_PROJECT\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un_ppfaAHv1J"
      },
      "source": [
        "## Task 2: Setting up RAG With Production in Mind\n",
        "\n",
        "This is the most crucial step in the process - in order to take advantage of:\n",
        "\n",
        "- Asyncronous requests\n",
        "- Parallel Execution in Chains\n",
        "- And more...\n",
        "\n",
        "You must...use LCEL. These benefits are provided out of the box and largely optimized behind the scenes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGi-db23JMAL"
      },
      "source": [
        "### Building our RAG Components: Retriever\n",
        "\n",
        "We'll start by building some familiar components - and showcase how they automatically scale to production features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvbT3HSDJemE"
      },
      "source": [
        "Please upload a PDF file to use in this example!\n",
        "\n",
        "> NOTE: If you're running this locally - you do not need to execute the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "dvYczNeY91Hn",
        "outputId": "c711c29b-e388-4d32-a763-f4504244eef2"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NtwoVUbaJlbW",
        "outputId": "5aa08bae-97c5-4f49-cb23-e9dbf194ecf7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'./DeepSeek_R1.pdf'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_path = \"./DeepSeek_R1.pdf\"\n",
        "file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kucGy3f0Jhdi"
      },
      "source": [
        "We'll define our chunking strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "G-DNvNFd8je5"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_zRRNcLKCZh"
      },
      "source": [
        "We'll chunk our uploaded PDF file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KOh6w9ud-ff6"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "Loader = PyMuPDFLoader\n",
        "loader = Loader(file_path)\n",
        "documents = loader.load()\n",
        "docs = text_splitter.split_documents(documents)\n",
        "for i, doc in enumerate(docs):\n",
        "    doc.metadata[\"source\"] = f\"source_{i}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4XLeqJMKGdQ"
      },
      "source": [
        "#### QDrant Vector Database - Cache Backed Embeddings\n",
        "\n",
        "The process of embedding is typically a very time consuming one - we must, for ever single vector in our VDB as well as query:\n",
        "\n",
        "1. Send the text to an API endpoint (self-hosted, OpenAI, etc)\n",
        "2. Wait for processing\n",
        "3. Receive response\n",
        "\n",
        "This process costs time, and money - and occurs *every single time a document gets converted into a vector representation*.\n",
        "\n",
        "Instead, what if we:\n",
        "\n",
        "1. Set up a cache that can hold our vectors and embeddings (similar to, or in some cases literally a vector database)\n",
        "2. Send the text to an API endpoint (self-hosted, OpenAI, etc)\n",
        "3. Check the cache to see if we've already converted this text before.\n",
        "  - If we have: Return the vector representation\n",
        "  - Else: Wait for processing and proceed\n",
        "4. Store the text that was converted alongside its vector representation in a cache of some kind.\n",
        "5. Return the vector representation\n",
        "\n",
        "Notice that we can shortcut some instances of \"Wait for processing and proceed\".\n",
        "\n",
        "Let's see how this is implemented in the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\"> Learnings\n",
        "\n",
        "<span style=\"color:green\">The Two Optimizations Working Together:\n",
        "\n",
        "- <span style=\"color:green\">Batching (batch_size=32): Reduces 73 API calls → 3 batch API calls\n",
        "- <span style=\"color:green\">Caching (CacheBackedEmbeddings): Reduces subsequent runs to 0 calls\n",
        "\n",
        "<span style=\"color:green\">Performance Comparison (API calls):\n",
        "| Approach | First Run | Second Run | Third Run |\n",
        "|----------|-----------|------------|-----------|\n",
        "| No batching, no caching | 73 calls | 73 calls | 73 calls |\n",
        "| Batching, no caching | 3 calls | 3 calls | 3 calls |\n",
        "| Batching + caching | 3 calls | 0 calls | 0 calls |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dzPUTCua98b2"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import Distance, VectorParams\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
        "import hashlib\n",
        "\n",
        "YOUR_EMBED_MODEL_URL = \"https://otuib8n7n9k6htjs.us-east-1.aws.endpoints.huggingface.cloud\"\n",
        "\n",
        "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
        "    model=YOUR_EMBED_MODEL_URL,\n",
        "    task=\"feature-extraction\",\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        ")\n",
        "\n",
        "# Get a fresh empty collection.\n",
        "collection_name = f\"pdf_to_parse_{uuid.uuid4()}\"\n",
        "\n",
        "# open connection to a local in memory qdrant instance.\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "# the size must match the size of the embeddings we're using.\n",
        "client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "# Normally, this is where we would just load all our documents into the vector store.\n",
        "# Instead, we'll do caching to avoid extra costs. \n",
        "# Our caching system will sit between our currently empty vector store and the embedding model.\n",
        "# Caching embeddings means we don't pay the cost of generating them from scratch every time.\n",
        "\n",
        "# Create a safe namespace by hashing the model URL (a unique identifier for our embedding model).\n",
        "safe_namespace = hashlib.md5(hf_embeddings.model.encode()).hexdigest()\n",
        "\n",
        "# Create a folder called cache in the current working directory. This is where \n",
        "# our cached embeddings will be stored.\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "# Create a CacheBackedEmbeddings object.\n",
        "# This object will cache our embeddings and store them in the folder we just created.\n",
        "# So we are going to use this object as our embedding model. It will check the cache first,\n",
        "# and if it doesn't find the embedding, it will generate it and store it in the cache.\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    hf_embeddings, store, namespace=safe_namespace, batch_size=32\n",
        ")\n",
        "\n",
        "\n",
        "# Typical QDrant Vector Store Set-up\n",
        "vectorstore = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=collection_name,\n",
        "    embedding=cached_embedder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<span style=\"color:green\"> So, after this we will have 73 files in cache/ Their names will be the namespace-hash+uuid-for-this-chunk. Files are kind of big! 17KB each. just storing the 768 floats would be about 6KB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a7eb3d9675804ac89a5496093ade9a9f',\n",
              " '26375ee218624f49989d8630c057bd0f',\n",
              " '11651a8d34e74c459743960bd0f7162c',\n",
              " '966eab6ef8cf480ba069667f56390411',\n",
              " '5860acc53a3b40cfa842899e574de759',\n",
              " '9023b7f90ff64a3b9dd76e2fb0f7050d',\n",
              " 'cf046687233b4bb8aa9bf627e3ff05a1',\n",
              " '27a27a653d6e43f0bbbc4d6d17396c11',\n",
              " '35355c3c582f4d0eb31665e013ec7fb7',\n",
              " '2772b9f4cb844081a096180894fc5cb5',\n",
              " 'ff030416c64b4fb683d33142c47c06a8',\n",
              " '218b9d7edcd34e8a8c3bcdda4e3e9e7f',\n",
              " '7c9acc54a9714c5e97cf781b7311602e',\n",
              " 'f68dc8b1e38e4839a3bb4d93bcf97877',\n",
              " '85de4f25d65d493ab34d7c8cd6e43c0e',\n",
              " 'b1381e9d7b554f00b13a83df6d92f5d5',\n",
              " '6ebf463e2fbd481cb80024b060a4d69f',\n",
              " '2822c8dcf313498a8a1966072eda592d',\n",
              " 'bf59bb50011e41dea2c8ad549caf740e',\n",
              " '21104b256c0d4b7f9d34a92e7d0ea8c9',\n",
              " '35b6ffb4db844130b1f46fe2565dd410',\n",
              " '5bec3dfb49794927b03934dcdd0c7668',\n",
              " 'ae83c069c4e54771ae96b6e2a8fc14de',\n",
              " '7030ddf827ee4d32ac21075fedd88891',\n",
              " '4ac36f9362994369916bad6bec0b7b2a',\n",
              " '66a1d904bc3246d5841f2f00d6d40770',\n",
              " 'ed5c09ea7cb2490591226cf67b3b3821',\n",
              " '2af6b5a2d2e943299634c6aed35fda7f',\n",
              " 'a0c1c705b69046a699319731d0186538',\n",
              " 'a6083f505d3e4effa7ffe2ea28a1ae30',\n",
              " '9d0fea1617344d4fa6302e6963f24372',\n",
              " '5b7a635b35164a4abecd71de3b6c12a0',\n",
              " '43170e778d0a4e63aa97494c7f0dd137',\n",
              " '3235f2a6adb7452bbc8eeff02086d842',\n",
              " 'c27409cee0f2474ea6c13b21cbfffc52',\n",
              " '581451388c6e48d497f4ed05657627a3',\n",
              " '15f4d986a805492ba5a89247029c2a14',\n",
              " 'cccd6d8ee61e4a36a9e13d652a86398c',\n",
              " '01fb7cb5cc994b739730dad30c5c9136',\n",
              " 'a45cfe1dd81e42ddb2105b660eee0f81',\n",
              " 'f6c2d58a3fa249268213d69c40f3666f',\n",
              " '58cfdfdc4be64463908678a307754720',\n",
              " 'e8cb9843ded54fc19ee2216b5e48e3bf',\n",
              " '8ddf9047e9284a648a03fad58ef799cd',\n",
              " 'fd068916cc1445cf9ab2b49a809c0952',\n",
              " '69fccbe3f87b4a7a8987f3b55aa00abd',\n",
              " 'd70e9d3533a246ddb3ab1a17ee4039f0',\n",
              " '038d3b71fb5743e293cdb2bbfdbe2dae',\n",
              " 'facc2f0475c44ef0821892dae1a19e97',\n",
              " 'aa6818e8159e44abbc2a32ced36d99f6',\n",
              " '3ce54cd614ef4b79902e3020cdf4d01b',\n",
              " '14b02264675e40a298a7513b37dec371',\n",
              " '2c22885eda1e4a13b81e14890e842b34',\n",
              " 'ab4ae31ca99f45838c2edf6eaea34337',\n",
              " 'a80eaeef88c74f2e9713bd28f7f70c22',\n",
              " '7f1960e94c514db1b832300def6c2090',\n",
              " '12caf89aa74d47be9e02fd76b7b11248',\n",
              " '3b1faa2e74b44017bcfe86d5a055f616',\n",
              " '0c838b4977f245b69677d1af01fdb320',\n",
              " 'c80c2e1bb7534e8194fdc36c2d3b7192',\n",
              " '944fdb25d9ce4ca5bf678f7aaa6dd417',\n",
              " 'd14cb84c8e59477a925c3c31da487cca',\n",
              " 'fc7d5913c50141dc94a26e9b2ea66a95',\n",
              " 'c6c2b42020ea46c18f7b4360ad87cb69',\n",
              " 'aac09432fbc04334ba1f16a292b0487f',\n",
              " '63f6f31877fc46958c1fc84a21793ca7',\n",
              " '20d5c158e3b64791ac9d8dba4b998dea',\n",
              " '0383edf928034f178a849d5944a1b274',\n",
              " 'c046db53b04e43d0b41a0d3564f04546',\n",
              " '108a0a535f4a4ef0aebdcfd8ce21e07f',\n",
              " 'e89beb6087f1479c951a2b88b97fc4a1',\n",
              " 'e127c958da684dee87e9d215b316b2f8',\n",
              " 'b618baa0ad75471facc58c05e6ce940a']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorstore.add_documents(docs) # this takes about 2.5 seconds for the 73 chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['d30e33f42a82469f87a5b1534e1deca8',\n",
              " '40e4b50d7c3241249c2c1fb88beee306',\n",
              " '529f92a58fe54d8e9e21060eac792e2b',\n",
              " '84f8c1a87e154444b711eb24bef18d3f',\n",
              " '28d02004517b448ca271f0b74c4ed793',\n",
              " 'bc3e44d03b2e47ef888f8e6ba3957c8d',\n",
              " '3f748a3b76e0437bb055bd59f28f24c4',\n",
              " 'e17992ebf4a540c2a48825c73e11f914',\n",
              " 'b1a070aa55d34e7a80087ef5fb48ed73',\n",
              " '56c15a9484164aebbf04d42ddedc7825',\n",
              " '6ccfc57f463c4e6fa71d164d2715696b',\n",
              " '1b7bf341a8394b79ae2229c22e0c0281',\n",
              " '689f33aa96db4be7bc19ab065833b695',\n",
              " '1154ac113f3b40b28d5a0de1aa301d28',\n",
              " '031bc374397b4173b12c5fae8f87676a',\n",
              " '4e09b83e7b524ec5a007c0761342e6f2',\n",
              " '33a25055cdb6405aa1054e308b421dd9',\n",
              " 'fe7bd28da82b41988c877a54b4402f4c',\n",
              " '3b1c2f1a69e148f1b74826952ece482e',\n",
              " '7387b52c3c6947f3b582db51bc512f41',\n",
              " '9ef8a78365a443c4977fe92ee59ea622',\n",
              " 'ece6521fa9cd41e8bee6ed3f23eb35e6',\n",
              " '7acebb1cf8d04b8dba66de9c16659be0',\n",
              " 'f31eccff71764a0e96e833b93c02f311',\n",
              " 'ca0507520e24430cb31b867a28289d70',\n",
              " '9a9a4436ab384070abc516c80dcc63b1',\n",
              " 'a3b28ccddc6a4b618068d7adffb23021',\n",
              " 'cef6e39431b3475097984795b41d4875',\n",
              " '88d276dc5e43483cbd89732f12831b6a',\n",
              " '3703cf7af8b14630ac64124b4a5c32ec',\n",
              " '8fc527d17a504788be48eeb3137930b0',\n",
              " '44aa0e333b0644ac9d37e1ac5cd336f2',\n",
              " '0f0cfd0d7fad424b9ab0a66711784ffc',\n",
              " '94fa4491b0134761b4a84e4a026f4cfd',\n",
              " 'b2608bbd2e4748d4aa67aac5fdb8777b',\n",
              " 'ad9d67c5febf4bceb5e9ecb06fbb64b6',\n",
              " 'bce7329d2107455c84689da9f4b0fdfe',\n",
              " 'a89302f5876843c6a5aa6abebb5e3a15',\n",
              " '95aecda5b7e941009a899e8afc91ce0f',\n",
              " '130e1a29da704544a6c54205e16b005b',\n",
              " '396896462b264f1ebfee75b073820b58',\n",
              " 'ca92a81777dc47a6b075a1df63f1d76e',\n",
              " '8b4b4208f61346aebe0130303b9e93c7',\n",
              " 'a7868afcce28455abacb1cf56c808b38',\n",
              " '08f3c662485344a0bd8ae72ff73530cc',\n",
              " 'fa2a8f7e076c4cb487a941e765ecdbda',\n",
              " '8447c0ee37e24d509596cc1938c890df',\n",
              " '28207ebb71d946aba0a50bc2f29285b0',\n",
              " 'd01f0b049963420793c918a762320383',\n",
              " '3532d1195cce4fb0b56fe5f881d3e35e',\n",
              " '5e7b51ce3ac84406be4eeb998de239ea',\n",
              " '90027d2a005c4bae8279b7ad091f5c0b',\n",
              " 'ad88d4236f4648e882a96344559f9199',\n",
              " 'c137b1fad4bf474e990d599bc5b412f7',\n",
              " '708c336871244b63a05b09ea995cf3a3',\n",
              " 'da92655fb323404b8b723dc25a8c3d51',\n",
              " 'b9d08e1c679f43eca3c55b071a3ec34f',\n",
              " 'd0a65ccaf6b34808914f91da5c2d1814',\n",
              " 'ceb68ba4cb7e4bd0b5d4e92be36f99c2',\n",
              " '5627c3f856b2483fa33eb7aac45e6d96',\n",
              " 'd59fe6147a0c4604a6f35e8e995e8a29',\n",
              " '4c471586922b4a96b2b6891ee58eec61',\n",
              " '5532dd0106594a15a620e2b9243ccf65',\n",
              " '37e34ea084e842ef93dee53019ad092e',\n",
              " 'd724dd4820e54eed9f8d0ab27104c86b',\n",
              " '1ab52214954b4096a1a71acff58cfce2',\n",
              " '3927d868d1704fa09559d190b5204945',\n",
              " '393832d3fcd7441ca6f818b36004db9c',\n",
              " '3e6937f110af4b72954b71aae8b18048',\n",
              " 'f7a727ab487f400e88c05817dbe8257c',\n",
              " '373a466285dc483cadc5951fd1569875',\n",
              " 'f4cab5685b8449cd9943be35be6793b7',\n",
              " '850e67acfb014ac5b83c1c2adc161772']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorstore.add_documents(docs) # this takes 0 seconds!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVZGvmNYLomp"
      },
      "source": [
        "##### ❓ Question #1:\n",
        "\n",
        "What are some limitations you can see with this approach? When is this most/least useful. Discuss with your group!\n",
        "\n",
        "##### <span style=\"color:green\"> Answer\n",
        "\n",
        "- <span style=\"color:green\"> Storage space! 17 KB for 73 chunks! It will be like 170GB or so for 10 million documents. Best to be aware of that.\n",
        "- <span style=\"color:green\"> Exact text match. \"cat sat on mat\" and Cat sat on the mat\" will be treated as different.\n",
        "- <span style=\"color:green\"> Invalidated if our embedding model gets updated or something?\n",
        "- <span style=\"color:green\"> LEAST USEFUL: What if we have content thats always changing - news stuff? or like real time data? Also, not good for small one-time use. If we were aggregating news of the day - this would make no difference. cache hit rate would be 0.\n",
        "- <span style=\"color:green\"> MOST USEFUL: same data, eval cycles, testing cycles - great!! Also, like a FAQ type usecase, especially with multi users - many of the same questions get asked! Perfect for that. Small updates to large documents - most chunks stay the same.\n",
        "\n",
        "> NOTE: There is no single correct answer here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZAOhyb3L9iD"
      },
      "source": [
        "##### 🏗️ Activity #1:\n",
        "\n",
        "Create a simple experiment that tests the cache-backed embeddings.\n",
        "\n",
        "# <span style=\"color:green\"> A simple Cache Hit Rate Test\n",
        "\n",
        "### <span style=\"color:green\">First Run (Mixed): 0.131 seconds\n",
        "-  **2 new texts** → Required API calls to Hugging Face\n",
        "-  **2 cached texts** → Instant retrieval from local files\n",
        "-  **Cache updated** → New embeddings saved for future use\n",
        "\n",
        "### <span style=\"color:green\">Second Run (All Cached): 0.001 seconds  \n",
        "-  **All 4 texts** → Found in cache immediately\n",
        "-  **Zero API calls** → No network requests needed\n",
        "-  **File reads only** → Lightning-fast local access\n",
        "\n",
        "<span style=\"color:green\">**Speedup**: **150x faster**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M_Mekif6MDqe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Cache Hit Rate Test ===\n",
            "\\n1. Testing mix of new and cached texts:\n",
            "Testing 2 new texts + 2 cached texts\n",
            "Mixed embedding time: 0.131 seconds\n",
            "\\n2. Re-embedding the same texts (should all be cached):\n",
            "All cached embedding time: 0.001 seconds\n",
            "Speedup: 150.0x faster\n",
            "\\n3. Cache file verification:\n",
            "Total cache files: 75\n",
            "Expected: 73 + 2 = 75\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "import time\n",
        "\n",
        "# Cache Hit Rate Test\n",
        "print(\"=== Cache Hit Rate Test ===\")\n",
        "\n",
        "# Test 1: Mix of new and already cached texts\n",
        "print(\"\\\\n1. Testing mix of new and cached texts:\")\n",
        "\n",
        "# These should already be in cache (from when we added docs to vectorstore)\n",
        "cached_texts = [docs[0].page_content, docs[1].page_content]\n",
        "\n",
        "# These are completely new texts\n",
        "new_texts = [\n",
        "    \"This is a completely new text that has never been embedded before\",\n",
        "    \"Another brand new sentence for testing cache misses\"\n",
        "]\n",
        "\n",
        "# Mix them together\n",
        "mixed_texts = new_texts + cached_texts\n",
        "\n",
        "print(f\"Testing {len(new_texts)} new texts + {len(cached_texts)} cached texts\")\n",
        "\n",
        "# Time the mixed embedding\n",
        "start_time = time.time()\n",
        "mixed_vectors = cached_embedder.embed_documents(mixed_texts)\n",
        "mixed_time = time.time() - start_time\n",
        "\n",
        "print(f\"Mixed embedding time: {mixed_time:.3f} seconds\")\n",
        "\n",
        "# Test 2: Now embed the same mixed texts again (all should be cached)\n",
        "print(\"\\\\n2. Re-embedding the same texts (should all be cached):\")\n",
        "\n",
        "start_time = time.time()\n",
        "cached_vectors = cached_embedder.embed_documents(mixed_texts)\n",
        "cached_time = time.time() - start_time\n",
        "\n",
        "print(f\"All cached embedding time: {cached_time:.3f} seconds\")\n",
        "print(f\"Speedup: {mixed_time/cached_time:.1f}x faster\")\n",
        "\n",
        "# Test 3: Check cache file count\n",
        "print(\"\\\\n3. Cache file verification:\")\n",
        "cache_files = [f for f in os.listdir(\"./cache\") if f.startswith(\"111101ee\")]\n",
        "print(f\"Total cache files: {len(cache_files)}\")\n",
        "print(f\"Expected: {len(docs)} + {len(new_texts)} = {len(docs) + len(new_texts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH0i-YovL8kZ"
      },
      "source": [
        "### Augmentation\n",
        "\n",
        "We'll create the classic RAG Prompt and create our `ChatPromptTemplates` as per usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WchaoMEx9j69"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "rag_system_prompt_template = \"\"\"\\\n",
        "You are a helpful assistant that uses the provided context to answer questions. Never reference this prompt, or the existence of context.\n",
        "\"\"\"\n",
        "\n",
        "# rag_message_list = [\n",
        "#     {\"role\" : \"system\", \"content\" : rag_system_prompt_template},\n",
        "# ]\n",
        "\n",
        "rag_user_prompt_template = \"\"\"\\\n",
        "Question:\n",
        "{question}\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", rag_system_prompt_template),\n",
        "    (\"human\", rag_user_prompt_template)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQKnByVWMpiK"
      },
      "source": [
        "### Generation\n",
        "\n",
        "Like usual, we'll set-up a `HuggingFaceEndpoint` model - and we'll use the fan favourite `Meta Llama 3.1 8B Instruct` for today.\n",
        "\n",
        "However, we'll also implement...a PROMPT CACHE!\n",
        "\n",
        "In essence, this works in a very similar way to the embedding cache - if we've seen this prompt before, we just use the stored response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fOXKkaY7ABab"
      },
      "outputs": [],
      "source": [
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "YOUR_LLM_ENDPOINT_URL = \"https://npbfssd0ai6866is.us-east-1.aws.endpoints.huggingface.cloud\"\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=f\"{YOUR_LLM_ENDPOINT_URL}\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=128,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhv8IqZoM9cY"
      },
      "source": [
        "Setting up the cache can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "thqam26gAyzN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.caches import InMemoryCache\n",
        "\n",
        "# Ok, this sets a GLOBAL cache that affects all LLM objects in LangChain. So, this does\n",
        "# apply now to our hf_llm object.\n",
        "set_llm_cache(InMemoryCache())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvxEovcEM_oA"
      },
      "source": [
        "##### ❓ Question #2:\n",
        "\n",
        "What are some limitations you can see with this approach? When is this most/least useful. Discuss with your group!\n",
        "##### <span style=\"color:green\"> Answer\n",
        "## <span style=\"color:green\">Limitations\n",
        "\n",
        "| Limitation | Problem |\n",
        "|------------|---------|\n",
        "| **Exact Match Only** | Minor text differences = cache miss |\n",
        "| **Memory Constraints** | InMemoryCache can exhaust RAM |\n",
        "| **No Persistence** | Cache lost on restart |\n",
        "| **Context Blind** | Same prompt, different contexts = same response |\n",
        "| **Kills Creativity** | Blocks randomness/variety |\n",
        "\n",
        "## <span style=\"color:green\">When Caching is **LEAST Useful**\n",
        "\n",
        "Creative content (poems, stories etc), real-time data, personalized responses, one-time queries.\n",
        "\n",
        "## <span style=\"color:green\">When Caching is **MOST Useful**\n",
        "\n",
        "multi-user FAQ style usage! Dev/testing/eval cycles, educational stuff, deterministic responses needed use cases.\n",
        "\n",
        "(this answer is looking similar to the previous one! I am missing some LLM specific excamples of usefulness maybe? But everything I thought of applies to both embedding and llms - except for creativity tasks I guess)\n",
        "\n",
        "\n",
        "## <span style=\"color:green\"> Golden Rule\n",
        "\n",
        "**Cache when**: Responses should be **consistent** and **reusable**\n",
        "\n",
        "**Don't cache when**: Users expect **variety** or **personalization**\n",
        "\n",
        "The key is matching caching strategy to user expectations!\n",
        "\n",
        "\n",
        "> NOTE: There is no single correct answer here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iCMjVYKNEeV"
      },
      "source": [
        "##### 🏗️ Activity #2:\n",
        "\n",
        "Create a simple experiment that tests the cache-backed generator.\n",
        "\n",
        "# <span style=\"color:green\"> LLM Cache Performance Test\n",
        "\n",
        "## <span style=\"color:green\">Experiment\n",
        "Tested the same prompt twice to measure cache performance:\n",
        "- Prompt: \"What is machine learning?\"\n",
        "- First call: API request to Hugging Face endpoint\n",
        "- Second call: Cache retrieval\n",
        "\n",
        "## <span style=\"color:green\">Results\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| First call time | 6.633 seconds |\n",
        "| Second call time | 0.000 seconds |\n",
        "| Performance improvement | 14,901x faster |\n",
        "| Response accuracy | Identical responses |\n",
        "\n",
        "There was probably some cold start penalty as well since my instance goes to sleep a lot! Although I tried again with \"what is silicon valley culture?\" and the speedup was the same!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QT5GfmsHNFqP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LLM Cache Test ===\n",
            "\n",
            "1. First call (should hit API):\n",
            "Time: 6.544 seconds\n",
            "Response:  Silicon Valley culture is a unique blend of innovation, entrepreneurship, and a relaxed, casual lif...\n",
            "\n",
            "2. Second call (should hit cache):\n",
            "Time: 0.000 seconds\n",
            "Response:  Silicon Valley culture is a unique blend of innovation, entrepreneurship, and a relaxed, casual lif...\n",
            "\n",
            "3. Results:\n",
            "First call: 6.544 seconds\n",
            "Second call: 0.000 seconds\n",
            "Speedup: 16079.0x faster\n",
            "Responses identical: True\n"
          ]
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "import time\n",
        "\n",
        "print(\"=== LLM Cache Test ===\")\n",
        "print()\n",
        "\n",
        "# Test the same prompt twice\n",
        "test_prompt = \"What is silicon valley culture?\"\n",
        "\n",
        "print(\"1. First call (should hit API):\")\n",
        "start_time = time.time()\n",
        "response1 = hf_llm.invoke(test_prompt)\n",
        "time1 = time.time() - start_time\n",
        "print(f\"Time: {time1:.3f} seconds\")\n",
        "print(f\"Response: {response1[:100]}...\")  # First 100 chars\n",
        "print()\n",
        "\n",
        "print(\"2. Second call (should hit cache):\")\n",
        "start_time = time.time()\n",
        "response2 = hf_llm.invoke(test_prompt)\n",
        "time2 = time.time() - start_time\n",
        "print(f\"Time: {time2:.3f} seconds\")\n",
        "print(f\"Response: {response2[:100]}...\")  # First 100 chars\n",
        "print()\n",
        "\n",
        "print(\"3. Results:\")\n",
        "print(f\"First call: {time1:.3f} seconds\")\n",
        "print(f\"Second call: {time2:.3f} seconds\")\n",
        "if time2 > 0:\n",
        "    print(f\"Speedup: {time1/time2:.1f}x faster\")\n",
        "else:\n",
        "    print(\"Second call was essentially instant!\")\n",
        "print(f\"Responses identical: {response1 == response2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyPnNWb9NH7W"
      },
      "source": [
        "## Task 3: RAG LCEL Chain\n",
        "\n",
        "We'll also set-up our typical RAG chain using LCEL.\n",
        "\n",
        "However, this time: We'll specifically call out that the `context` and `question` halves of the first \"link\" in the chain are executed *in parallel* by default!\n",
        "\n",
        "Thanks, LCEL!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3JNvSsx_CEtI"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables.passthrough import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "        | chat_prompt | hf_llm\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx--wVctNdGa"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43uQegbnDQKP",
        "outputId": "a9ff032b-4eb2-4f5f-f456-1fc6aa24aaec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Human: Here are 50 things about this document:\\n\\n1. The document is a PDF.\\n2. The document has 22 pages.\\n3. The document was created on January 23, 2025.\\n4. The document was modified on January 23, 2025.\\n5. The document's title is empty.\\n6. The document's author is empty.\\n7. The document's subject is empty.\\n8. The document's keywords are empty.\\n9. The document's creator is LaTeX with hyperref.\\n10. The document's producer is pdfTeX-1.40.26.\\n11. The document's creation\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"Write 50 things about this document!\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tYAvHrJNecy"
      },
      "source": [
        "##### 🏗️ Activity #3:\n",
        "\n",
        "Show, through LangSmith, the different between a trace that is leveraging cache-backed embeddings and LLM calls - and one that isn't.\n",
        "\n",
        "So, I ran the above cell twice and the first time it took over 6 seconds and the second time it took 0.1 seconds!!\n",
        "\n",
        "## <span style=\"color:green\"> LangSmith Trace Comparison Results\n",
        "\n",
        "Here is a simple screenshot (detailed one at the end)\n",
        "So, latency went down from 7.24 seconds to 0.12 seconds and number of tokens went down from 819 to 0.\n",
        "In the second run, remember that we paid no api costs - not to embedding model and not to the llm model\n",
        "\n",
        "\n",
        "![LangSmith Run Comparison](simple_ls.png)\n",
        "\n",
        "<span style=\"color:green\">The trace comparison clearly shows how cache-backed embeddings and LLM caching provide significant performance improvements for repeated queries.\n",
        "\n",
        " <span style=\"color:green\">Here is a more detailed comparison.(Left is no caching, right is with caching)\n",
        "![LangSmith Run Comparison](ls_run_compare.png)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
